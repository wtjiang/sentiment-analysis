{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def segmentWords(s): \n",
    "    return s.split()\n",
    "\n",
    "def readFile(fileName):\n",
    "    # Function for reading file\n",
    "    # input: filename as string\n",
    "    # output: contents of file as list containing single words\n",
    "    contents = []\n",
    "    f = open(fileName)\n",
    "    for line in f:\n",
    "        contents.append(line)\n",
    "    f.close()\n",
    "    result = segmentWords('\\n'.join(contents))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Dataframe containing the counts of each word in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = []\n",
    "\n",
    "for c in os.listdir(\"data_training\"):\n",
    "    directory = \"data_training/\" + c\n",
    "    for file in os.listdir(directory):\n",
    "        words = readFile(directory + \"/\" + file)\n",
    "        e = {x:words.count(x) for x in words}\n",
    "        e['__FileID__'] = file\n",
    "        e['__CLASS__'] = c\n",
    "        d.append(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe from d - make sure to fill all the nan values with zeros.\n",
    "\n",
    "References:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(data=d)\n",
    "df = df.fillna(value = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into training and validation set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Sample 80% of your dataframe to be the training data\n",
    "\n",
    "* Let the remaining 20% be the validation data (you can filter out the indicies of the original dataframe that weren't selected for the training data)\n",
    "\n",
    "References:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = df.sample(frac=0.8)\n",
    "df_valid = df.drop(df_train.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Split the dataframe for both training and validation data into x and y dataframes - where y contains the labels and x contains the words\n",
    "\n",
    "References:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = df_train.drop(labels =['__FileID__','__CLASS__'], axis = 1)\n",
    "train_y = df_train['__CLASS__']\n",
    "\n",
    "valid_x = df_valid.drop(labels =['__FileID__','__CLASS__'], axis = 1)\n",
    "valid_y = df_valid['__CLASS__']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Logistic Regression\n",
    "* Use sklearn's linear_model.LogisticRegression() to create your model.\n",
    "* Fit the data and labels with your model.\n",
    "* Score your model with the same data and labels.\n",
    "\n",
    "References:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Prediction for training data\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(train_x, train_y)\n",
    "logreg.score(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.63636364  0.75757576  0.65625     0.78125     0.78125     0.78125\n",
      "  0.9375      0.65625     0.67741935  0.90322581]\n",
      "average score: 0.7568334555229718\n"
     ]
    }
   ],
   "source": [
    "#cross-validation\n",
    "scores = cross_val_score(logreg, valid_x, valid_y, cv=10)\n",
    "print(scores)\n",
    "print('average score: {}'.format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the test accuracy of 100% vs. the average validation accuracy of 75.9%, it is very clear that there is a case of overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.001, penalty = l1, Train score = 0.514,  Test score = 0.447\n",
      "C = 0.010, penalty = l1, Train score = 0.684,  Test score = 0.675\n",
      "C = 0.100, penalty = l1, Train score = 0.902,  Test score = 0.822\n",
      "C = 1.000, penalty = l1, Train score = 1.000,  Test score = 0.828\n",
      "C = 0.001, penalty = l2, Train score = 0.869,  Test score = 0.809\n",
      "C = 0.010, penalty = l2, Train score = 0.993,  Test score = 0.853\n",
      "C = 0.100, penalty = l2, Train score = 1.000,  Test score = 0.866\n",
      "C = 1.000, penalty = l2, Train score = 1.000,  Test score = 0.866\n"
     ]
    }
   ],
   "source": [
    "for p in ['l1', 'l2']:\n",
    "    for c in [0.001, 0.01, 0.1, 1]:\n",
    "        lr_param = LogisticRegression(C = c, penalty = p)\n",
    "        lr_param.fit(train_x, train_y)\n",
    "        print(\"C = \" + str(format(c, '.3f')) + \", penalty = \" + str(p) + \", Train score = \" + \n",
    "              str(format(lr_param.score(train_x,train_y), '.3f'))\n",
    "             + \",  Test score = \" + str(format(lr_param.score(valid_x,valid_y), '.3f')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection\n",
    "* In the backward stepsize selection method, you can remove coefficients and the corresponding x columns, where the coefficient is more than a particular amount away from the mean - you can choose how far from the mean is reasonable.\n",
    "\n",
    "References:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html#\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.where.html\n",
    "https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.std.html\n",
    "https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.mean.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.001, penalty = l1, Train score = 0.514,  Test score = 0.447\n",
      "C = 0.010, penalty = l1, Train score = 0.696,  Test score = 0.666\n",
      "C = 0.100, penalty = l1, Train score = 0.900,  Test score = 0.819\n",
      "C = 1.000, penalty = l1, Train score = 1.000,  Test score = 0.825\n",
      "C = 0.001, penalty = l2, Train score = 0.863,  Test score = 0.784\n",
      "C = 0.010, penalty = l2, Train score = 0.993,  Test score = 0.866\n",
      "C = 0.100, penalty = l2, Train score = 1.000,  Test score = 0.863\n",
      "C = 1.000, penalty = l2, Train score = 1.000,  Test score = 0.844\n"
     ]
    }
   ],
   "source": [
    "weights = (np.array(logreg.coef_[0])-np.mean(logreg.coef_[0]))/np.std(logreg.coef_[0])\n",
    "train_x_bss = train_x.iloc[:, np.where(np.abs(weights)>1)[0].tolist()]\n",
    "test_x_bss = valid_x.iloc[:, np.where(np.abs(weights)>1)[0].tolist()]\n",
    "\n",
    "for p in ['l1', 'l2']:\n",
    "    for c in [0.001, 0.01, 0.1, 1]:\n",
    "        lr3 = LogisticRegression(C=c, penalty=p)\n",
    "        lr3.fit(train_x_bss, train_y)\n",
    "        print(\"C = \" + str(format(c, '.3f')) + \", penalty = \" + str(p) + \", Train score = \" + \n",
    "                  str(format(lr3.score(train_x_bss, train_y), '.3f'))\n",
    "                 + \",  Test score = \" + str(format(lr3.score(test_x_bss, valid_y), '.3f')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did you select which features to remove? Why did that reduce overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Features were removed based on weight. Based on the results seen above, C = 0.010, and penalty = l2 proved to perform well and decrease overfitting on the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Decision Tree\n",
    "\n",
    "* Initialize your model as a decision tree with sklearn.\n",
    "* Fit the data and labels to the model.\n",
    "\n",
    "References:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.603125\n"
     ]
    }
   ],
   "source": [
    "dtree = DecisionTreeClassifier() \n",
    "dtree = dtree.fit(train_x, train_y)\n",
    "print(\"Train accuracy: \" + str(dtree.score(train_x, train_y)))\n",
    "print(\"Test accuracy: \" + str(dtree.score(valid_x, valid_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing Parameters\n",
    "* To test out which value is optimal for a particular parameter, you can either loop through various values or look into sklearn.model_selection.GridSearchCV\n",
    "\n",
    "References:\n",
    "\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did you choose which parameters to change and what value to give to them? Feel free to show a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  return getattr(obj, method)(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "c = np.shape(train_y)\n",
    "r = np.shape(train_y)\n",
    "train_y_reshape = np.reshape(train_y, (c))\n",
    "\n",
    "params = {\"max_leaf_nodes\": [50,100,150,200]}\n",
    "gscv = model_selection.GridSearchCV(dtree, params)\n",
    "gscv = gscv.fit(train_x.values, train_y_reshape.values.reshape(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/alex/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/alex/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/alex/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/alex/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 6.8609492 ,  7.68731435,  8.21607272,  8.18915423]),\n",
       " 'mean_score_time': array([ 0.0274121 ,  0.02619664,  0.02625505,  0.02632125]),\n",
       " 'mean_test_score': array([ 0.63828125,  0.6453125 ,  0.64140625,  0.63984375]),\n",
       " 'mean_train_score': array([ 0.91483601,  0.99843689,  1.        ,  1.        ]),\n",
       " 'param_max_leaf_nodes': masked_array(data = [50 100 150 200],\n",
       "              mask = [False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': [{'max_leaf_nodes': 50},\n",
       "  {'max_leaf_nodes': 100},\n",
       "  {'max_leaf_nodes': 150},\n",
       "  {'max_leaf_nodes': 200}],\n",
       " 'rank_test_score': array([4, 1, 2, 3], dtype=int32),\n",
       " 'split0_test_score': array([ 0.64485981,  0.64485981,  0.64252336,  0.64953271]),\n",
       " 'split0_train_score': array([ 0.90492958,  0.99765258,  1.        ,  1.        ]),\n",
       " 'split1_test_score': array([ 0.64084507,  0.64553991,  0.65023474,  0.64319249]),\n",
       " 'split1_train_score': array([ 0.91217799,  0.99765808,  1.        ,  1.        ]),\n",
       " 'split2_test_score': array([ 0.62910798,  0.64553991,  0.6314554 ,  0.62676056]),\n",
       " 'split2_train_score': array([ 0.92740047,  1.        ,  1.        ,  1.        ]),\n",
       " 'std_fit_time': array([ 0.16942132,  0.11975662,  0.53929729,  0.60141962]),\n",
       " 'std_score_time': array([  2.43155737e-03,   7.48660491e-05,   4.42085068e-04,\n",
       "          3.88070279e-04]),\n",
       " 'std_test_score': array([ 0.00668314,  0.00032085,  0.00770145,  0.00959632]),\n",
       " 'std_train_score': array([ 0.00936426,  0.00110529,  0.        ,  0.        ])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [50,100,150,200]\n",
    "y = [tree.DecisionTreeClassifier(max_depth=k).fit(train_x,train_y_reshape).score(valid_x,valid_y) for k in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGIZJREFUeJzt3X9wXeWd3/H3JxIOmCRYLLJxbbeYxm6GZBaDb0zacVs3\nKYuAHZv8EcbQBG9Lx7BTGJLJkHWyoSEz2xkW8mv/AIxj3HWbNB66cYqGYXEIi+3Z2YVIIsaxEQ4q\nAVte25I3kyZxd+I1/vSP+6h7cyOhK+naV4o/rxnNPed5nnPu9xhdfbjn3Psc2SYiIuIdrS4gIiKm\nhwRCREQACYSIiCgSCBERASQQIiKiSCBERATQYCBI6pJ0QNKApA1jjFklaY+k/ZJ21bS/IemHpa+3\npv1iSc9Keq08dkz9cCIiYrI03vcQJLUBPwKuBQaBHuAW26/UjJkD/BXQZfugpLm2h0rfG0DF9vG6\n/T4I/MT2AyVkOmz/QfMOLSIiJqKRdwgrgAHbr9s+CWwD1tSNuRXYbvsgwEgYjGMNsLUsbwVuaqzk\niIg4E9obGLMAOFSzPghcUzdmKXCepJ3Au4E/sf3fSp+B70l6C3jM9qbSPs/2kbJ8FJg32pNLWg+s\nB7jwwguXv+9972ug5IiIGNHX13fcdud44xoJhEa0A8uBjwAXAH8t6QXbPwJW2j4saS7wrKRXbe+u\n3di2JY167qoEyCaASqXi3t7e0YZFRMQYJL3ZyLhGThkdBhbVrC8sbbUGgR22T5RrBbuBKwFsHy6P\nQ8B3qJ6CAjgmaX4pdj7QyGmmiIg4QxoJhB5giaTFkmYBa4HuujFPAisltUuaTfWUUr+kCyW9G0DS\nhcDvAPvKNt3AurK8ruwjIiJaZNxTRrZPSboL2AG0AVts75d0Z+nfaLtf0jPAXuA0sNn2PkmXA9+R\nNPJc/8P2M2XXDwBPSLodeBO4udkHFxERjRv3Y6fTSa4hRERMnKQ+25XxxuWbyhERASQQIiKiSCBE\nRASQQIiIiCKBEBERQAIhIiKKBEJERAAJhIiIKBIIEREBJBAiIqJIIEREBJBAiIiIIoEQERFAAiEi\nIooEQkREAAmEiIgoEggREQEkECIiomgoECR1STogaUDShjHGrJK0R9J+Sbvq+tok/UDSUzVt90s6\nXLbZI+mGqR1KRERMRft4AyS1AQ8D1wKDQI+kbtuv1IyZAzwCdNk+KGlu3W7uAfqB99S1f9X2l6Zy\nABER0RyNvENYAQzYft32SWAbsKZuzK3AdtsHAWwPjXRIWgjcCGxuTskREXEmNBIIC4BDNeuDpa3W\nUqBD0k5JfZJuq+n7GvAZ4PQo+75b0l5JWyR1jPbkktZL6pXUOzw83EC5ERExGc26qNwOLKf6TuA6\n4D5JSyX9LjBku2+UbR4FLgeWAUeAL4+2Y9ubbFdsVzo7O5tUbkRE1Bv3GgJwGFhUs76wtNUaBP7W\n9gnghKTdwJXA1cDqcsH4fOA9kr5h++O2j41sLOnrwFNERETLNPIOoQdYImmxpFnAWqC7bsyTwEpJ\n7ZJmA9cA/bY/a3uh7cvKdn9h++MAkubXbP9RYN8UjyUiIqZg3HcItk9JugvYAbQBW2zvl3Rn6d9o\nu1/SM8BeqtcKNtse7w/8g5KWAQbeAO6YwnFERMQUyXara2hYpVJxb29vq8uIiJhRJPXZrow3Lt9U\njogIIIEQERFFAiEiIoAEQkREFAmEiIgAEggREVEkECIiAkggREREkUCIiAgggRAREUUCISIigARC\nREQUCYSIiAASCBERUSQQIiICSCBERESRQIiICCCBEBERRUOBIKlL0gFJA5I2jDFmlaQ9kvZL2lXX\n1ybpB5Keqmm7WNKzkl4rjx1TO5SIiJiKcQNBUhvwMHA9cAVwi6Qr6sbMAR4BVtt+P/Cxut3cA/TX\ntW0AnrO9BHiurEdERIs08g5hBTBg+3XbJ4FtwJq6MbcC220fBLA9NNIhaSFwI7C5bps1wNayvBW4\naeLlR0REszQSCAuAQzXrg6Wt1lKgQ9JOSX2Sbqvp+xrwGeB03TbzbB8py0eBeaM9uaT1knol9Q4P\nDzdQbkRETEZ7E/ezHPgIcAHw15JeoBoUQ7b7JK0aa2PbluQx+jYBmwAqlcqoYyIiYuoaCYTDwKKa\n9YWlrdYg8Le2TwAnJO0GrgSuBlZLugE4H3iPpG/Y/jhwTNJ820ckzQeGiIiIlmnklFEPsETSYkmz\ngLVAd92YJ4GVktolzQauAfptf9b2QtuXle3+ooQBZR/ryvK6so+IiGiRcd8h2D4l6S5gB9AGbLG9\nX9KdpX+j7X5JzwB7qV4r2Gx73zi7fgB4QtLtwJvAzVM5kIiImBrZM+e0fKVScW9vb6vLiIiYUST1\n2a6MNy7fVI6ICCCBEBERRQIhIiKABEJERBQJhIiIABIIERFRJBAiIgJIIERERJFAiIgIIIEQERFF\nAiEiIoAEQkREFAmEiIgAEggREVEkECIiAkggREREkUCIiAigwUCQ1CXpgKQBSRvGGLNK0h5J+yXt\nKm3nS/q+pJdL+xdrxt8v6XDZZo+kG5pzSBERMRnj3lNZUhvwMHAtMAj0SOq2/UrNmDnAI0CX7YOS\n5pauXwIftv0LSecBfynpz22/UPq/avtLzTygiIiYnEbeIawABmy/bvsksA1YUzfmVmC77YMAtofK\no23/oow5r/zMnJs4R0ScQxoJhAXAoZr1wdJWaynQIWmnpD5Jt410SGqTtAcYAp61/WLNdndL2itp\ni6SOSR5DREQ0QbMuKrcDy4EbgeuA+yQtBbD9lu1lwEJghaQPlG0eBS4HlgFHgC+PtmNJ6yX1Suod\nHh5uUrkxHfR0P8bR+9/L6S9cxNH730tP92OtLininNZIIBwGFtWsLyxttQaBHbZP2D4O7AaurB1g\n+6fA80BXWT9WwuI08HWqp6Z+je1Ntiu2K52dnY0cU8wAPd2P8YG+z3Mpw7xDcCnDfKDv8wmFiBZq\nJBB6gCWSFkuaBawFuuvGPAmslNQuaTZwDdAvqbNccEbSBVQvTL9a1ufXbP9RYN/UDiVmkkUvPcQF\nOvkrbRfoJIteeqhFFUXEuJ8ysn1K0l3ADqAN2GJ7v6Q7S/9G2/2SngH2AqeBzbb3SfptYGv5pNI7\ngCdsP1V2/aCkZVQvMr8B3NHsg4vpa66HQaO1Hz/7xUQE0EAgANh+Gni6rm1j3fpDwEN1bXuBq8bY\n5ycmVGn8RhlSJ5fy69eEhnQJl7agnojIN5WjRQ5dfS9/51m/0vZ3nsWhq+9tUUURkUCIlvjg6jvY\nt/yPOEonpy2O0sm+5X/EB1fnzGFEq8ieOd8Tq1Qq7u3tbXUZEREziqQ+25XxxuUdQkREAAmEiIgo\nEggREQEkECIiokggREQEkECIiIgigRAREUACISIiigRCREQADU5uFxERZ09P92Mseukh5nqYIXVy\n6Op7z8q0LgmEiIhpZOTmURfoJJSbR13U93l64IyHQk4ZRURMI628eVQCISJiGpnr0e8dfzZuHpVA\niIiYRoY0+r3jh3TJGX/uBEJExDTSyptHNRQIkrokHZA0IGnDGGNWSdojab+kXaXtfEnfl/Ryaf9i\nzfiLJT0r6bXy2NGcQ4qImLlaefOocW+QI6kN+BFwLTAI9AC32H6lZswc4K+ALtsHJc21PSRJwIW2\nfyHpPOAvgXtsvyDpQeAnth8oIdNh+w/erpbcICciYuKaeYOcFcCA7ddtnwS2AWvqxtwKbLd9EMD2\nUHm07V+UMeeVn5EEWgNsLctbgZsaqCUiIs6QRgJhAXCoZn2wtNVaCnRI2impT9JtIx2S2iTtAYaA\nZ22/WLrm2T5Slo8C80Z7cknrJfVK6h0eHv3qe0RETF2zLiq3A8uBG4HrgPskLQWw/ZbtZcBCYIWk\nD9Rv7Op5q1HPXdneZLtiu9LZOfrV94iImLpGAuEwsKhmfWFpqzUI7LB9wvZxYDdwZe0A2z8Fnge6\nStMxSfMByuPQxMuPiIhmaSQQeoAlkhZLmgWsBbrrxjwJrJTULmk2cA3QL6mzXHBG0gVUL0y/Wrbp\nBtaV5XVlHxER0SLjzmVk+5Sku4AdQBuwxfZ+SXeW/o22+yU9A+wFTgObbe+T9NvA1vJJpXcAT9h+\nquz6AeAJSbcDbwI3N/3oIiKiYeN+7HQ6ycdOIyImrpkfO42IiHPAOTX9davmGI+ImAnOmUBo5Rzj\nEREzwTlzyqiVc4xHRMwE50wgtHKO8YiImeCcCYRWzjEeETETnDOB0Mo5xiMiZoJzJhBaOcd4RMRM\nkC+mRUT8hssX0yIiYkISCBERASQQIiKiSCBERASQQIiIiCKBEBERQAIhIiKKBEJERAAJhIiIKBoK\nBEldkg5IGpC0YYwxqyTtkbRf0q7StkjS85JeKe331Iy/X9Lhss0eSTc055AiYqJ6uh/j6P3v5fQX\nLuLo/e+lp/uxVpcULTDuDXIktQEPA9cCg0CPpG7br9SMmQM8AnTZPihpbuk6BXza9kuS3g30SXq2\nZtuv2v5SMw8oIiYmN4+KEY28Q1gBDNh+3fZJYBuwpm7MrcB22wcBbA+VxyO2XyrLPwf6gQXNKj4i\npi43j4oRjQTCAuBQzfogv/5HfSnQIWmnpD5Jt9XvRNJlwFXAizXNd0vaK2mLpI7RnlzSekm9knqH\nh0e/yU1ETF5uHhUjmnVRuR1YDtwIXAfcJ2npSKekdwHfBj5p+2el+VHgcmAZcAT48mg7tr3JdsV2\npbNz9JvcRMTk5eZRMaKRQDgMLKpZX1jaag0CO2yfsH0c2A1cCSDpPKph8E3b20c2sH3M9lu2TwNf\np3pqKiLOstw8KkY0Egg9wBJJiyXNAtYC3XVjngRWSmqXNBu4BuiXJOBxoN/2V2o3kDS/ZvWjwL7J\nHkRETF5uHhUjxv2Uke1Tku4CdgBtwBbb+yXdWfo32u6X9AywFzgNbLa9T9JK4BPADyXtKbv8nO2n\ngQclLQMMvAHkty+iRT64+g4oAXBp+YlzT+6YFhHxGy53TIuIiAlJIEREBJBAiIiIIoEQERFAAiEi\nIooEQkREAAmEiIgoEggREQEkECIiokggREQEkECIiIgigRAREUACISIiigRCREQACYSIiCgSCBER\nASQQIiKiSCBERATQYCBI6pJ0QNKApA1jjFklaY+k/ZJ2lbZFkp6X9Eppv6dm/MWSnpX0WnnsaM4h\nRUTEZIwbCJLagIeB64ErgFskXVE3Zg7wCLDa9vuBj5WuU8CnbV8BfAj4TzXbbgCes70EeK6sR0RE\nizTyDmEFMGD7ddsngW3AmroxtwLbbR8EsD1UHo/Yfqks/xzoBxaUbdYAW8vyVuCmqRxIRERMTSOB\nsAA4VLM+yD/8UR+xFOiQtFNSn6Tb6nci6TLgKuDF0jTP9pGyfBSYN9qTS1ovqVdS7/DwcAPlRkTE\nZDTronI7sBy4EbgOuE/S0pFOSe8Cvg180vbP6je2bcCj7dj2JtsV25XOzs4mlRsREfUaCYTDwKKa\n9YWlrdYgsMP2CdvHgd3AlQCSzqMaBt+0vb1mm2OS5pcx84GhyR1CREQ0QyOB0AMskbRY0ixgLdBd\nN+ZJYKWkdkmzgWuAfkkCHgf6bX+lbptuYF1ZXlf2ERERLdI+3gDbpyTdBewA2oAttvdLurP0b7Td\nL+kZYC9wGthse5+klcAngB9K2lN2+TnbTwMPAE9Iuh14E7i56UcXERENU/X0/cxQqVTc29vb6jIi\nImYUSX22K+ONyzeVIyICSCBERESRQIiICCCBEBERRQIhIiKABEJERBQJhIiIABIIERFRJBAiIgJI\nIERERJFAiIgIIIEQERFFAiEiIoAEQkREFAmEiIgAEggREVEkECIiAmgwECR1STogaUDShjHGrJK0\nR9J+Sbtq2rdIGpK0r278/ZIOl232SLphaocSERFTMW4gSGoDHgauB64AbpF0Rd2YOcAjwGrb7wc+\nVtP9p0DXGLv/qu1l5efpSdQfERFN0sg7hBXAgO3XbZ8EtgFr6sbcCmy3fRDA9tBIh+3dwE+aVG9E\nRJwhjQTCAuBQzfpgaau1FOiQtFNSn6TbGnz+uyXtLaeVOhrcJiIizoBmXVRuB5YDNwLXAfdJWjrO\nNo8ClwPLgCPAl0cbJGm9pF5JvcPDw00qNyIi6jUSCIeBRTXrC0tbrUFgh+0Tto8Du4Er326nto/Z\nfsv2aeDrVE9NjTZuk+2K7UpnZ2cD5UZExGQ0Egg9wBJJiyXNAtYC3XVjngRWSmqXNBu4Buh/u51K\nml+z+lFg31hjIyLizGsfb4DtU5LuAnYAbcAW2/sl3Vn6N9rul/QMsBc4DWy2vQ9A0reAVcAlkgaB\nL9h+HHhQ0jLAwBvAHU0/uoiIaJhst7qGhlUqFff29ra6jIiIGUVSn+3KeOPyTeWIiAASCBERUSQQ\nIiICSCBERESRQIiICCCBEBERRQIhIiKABEJERBQJhIiIABIIERFRJBAiIgJIIERERJFAiIgIIIEQ\nERFFAiEiIoAEQkREFAmEiIgAEggREVE0FAiSuiQdkDQgacMYY1ZJ2iNpv6RdNe1bJA1J2lc3/mJJ\nz0p6rTx2TO1QIiJiKsYNBEltwMPA9cAVwC2SrqgbMwd4BFht+/3Ax2q6/xToGmXXG4DnbC8Bnivr\nERHRIo28Q1gBDNh+3fZJYBuwpm7MrcB22wcBbA+NdNjeDfxklP2uAbaW5a3ATROsPSIimqiRQFgA\nHKpZHyxttZYCHZJ2SuqTdFsD+51n+0hZPgrMG22QpPWSeiX1Dg8PN7DbiIiYjGZdVG4HlgM3AtcB\n90la2ujGtg14jL5Ntiu2K52dnU0pNiIifl0jgXAYWFSzvrC01RoEdtg+Yfs4sBu4cpz9HpM0H6A8\nDo0zPiIizqBGAqEHWCJpsaRZwFqgu27Mk8BKSe2SZgPXAP3j7LcbWFeW15V9REREi6h6tmacQdIN\nwNeANmCL7f8i6U4A2xvLmHuBfw+cBjbb/lpp/xawCrgEOAZ8wfbjkn4LeAL4x8CbwM22R7v4XFvH\ncBk73VwCHG91EW8j9U3edK4NUt9UnSv1/RPb455zbygQ4u1J6rVdaXUdY0l9kzeda4PUN1Wp71fl\nm8oREQEkECIiokggNMemVhcwjtQ3edO5Nkh9U5X6auQaQkREAHmHEBERRQIhIiKABMKESZoj6c8k\nvSqpX9I/n05TeUv6VJmCfJ+kb0k6v5X1jTb9+dvVI+mzZZr1A5Kua1F9D5X/vnslfafM5jtt6qvp\n+7QkS7pkutUn6e7yb7hf0oOtqG+M/7bLJL1QpurvlbSiFbWV51sk6XlJr5R/p3tKe+teH7bzM4Ef\nqjOz/seyPAuYAzwIbChtG4A/blFtC4AfAxeU9SeA32tlfcC/Aq4G9tW0jVoP1enVXwbeCSwG/jfQ\n1oL6fgdoL8t/PN3qK+2LgB1Uv6h5yXSqD/g3wPeAd5b1ua2ob4zavgtcX5ZvAHa28N9uPnB1WX43\n8KNSR8teH3mHMAGSLqL6S/Y4gO2Ttn/K9JrKux24QFI7MBv4G1pYn0ef/nysetYA22z/0vaPgQGq\n06+f1fpsf9f2qbL6AtX5u6ZNfcVXgc/wq5NCTpf6fh94wPYvy5iRecrOan1j1GbgPWX5Iqqvj7Ne\nW6nviO2XyvLPqU73s4AWvj4SCBOzGBgG/qukH0jaLOlCGpzK+0yzfRj4EnAQOAL8H9vfnS711Rir\nnkamWj/b/gPw52V5WtQnaQ1w2PbLdV3Toj6q0+H/S0kvStol6YOlfTrU90ngIUmHqL5WPjsdapN0\nGXAV8CItfH0kECamnepb0EdtXwWcoO5Ob66+t2vJZ3nLucY1VIPrHwEXSvp47ZhW1jea6VZPLUl/\nCJwCvtnqWkaUySM/B/znVtfyNtqBi4EPAfcCT0hSa0v6/34f+JTtRcCnKO/2W0nSu4BvA5+0/bPa\nvrP9+kggTMwgMGj7xbL+Z1QDYrpM5f1vgR/bHrb998B24F9Mo/pGjFVPI1OtnxWSfg/4XeDflRcl\nTI/6/inVwH9Z0hulhpckXTpN6oPq62S7q75PdcLLS6ZJfeuovi4A/if/cMqlJbVJOo9qGHzT9khd\nLXt9JBAmwPZR4JCkf1aaPgK8wvSZyvsg8CFJs8v/kX2E6nnJ6VLfiLHq6QbWSnqnpMXAEuD7Z7s4\nSV1Uz8+vtv1/a7paXp/tH9qea/sy25dR/eN7dfndbHl9xf+iemEZVW+UNYvqjJ3Tob6/Af51Wf4w\n8FpZPuu1ldfo40C/7a/UdLXu9XEmr6L/Jv4Ay4BeYC/VX/wO4LeA56j+cn0PuLiF9X0ReBXYB/x3\nqp9IaFl9wLeoXs/4e6p/vG5/u3qAP6T66YkDlE+DtKC+AarnaveUn43Tqb66/jconzKaLvVRDYBv\nlN/Bl4APt6K+MWpbCfRR/bTOi8DyFv7braR6Omhvze/aDa18fWTqioiIAHLKKCIiigRCREQACYSI\niCgSCBERASQQIiKiSCBERASQQIiIiOL/ATad0EERquEMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5640b58d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is a single decision tree so prone to overfitting?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A single decision tree is so prone to overfitting because it has many branches that learn the training set too well, optimizing training set error at the cost of test set error. When the decision tree is faced with fitting a more generalized test set, its branches do not fit the test set as well as the training set, as they have been too specifically tailored to the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Random Forest\n",
    "\n",
    "* Use sklearn's ensemble.RandomForestClassifier() to create your model.\n",
    "* Fit the data and labels with your model.\n",
    "* Score your model with the same data and labels.\n",
    "\n",
    "References:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default score for training data: 0.9921875\n",
      "Default score for test data: 0.65\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(train_x, train_y)\n",
    "print(\"Default score for training data: \" + str(clf.score(train_x, train_y)))\n",
    "print(\"Default score for test data: \" + str(clf.score(valid_x, valid_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default score for training data: 1.0\n",
      "Default score for test data: 0.815625\n"
     ]
    }
   ],
   "source": [
    "clf.set_params(n_estimators=400, criterion=\"entropy\")\n",
    "clf.fit(train_x, train_y)\n",
    "print(\"Default score for training data: \" + str(clf.score(train_x, train_y)))\n",
    "print(\"Default score for test data: \" + str(clf.score(valid_x, valid_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What parameters did you choose to change and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I set n_estimators to 400 because this creates more estimators, and the model at the end that merges all the different estimators will be better because outliers will disappear and the best features are selected. I also chose to use information gain (\"entropy\") to measure the quality of splits because it is better for attributes that occur in classes. Changing the other parameters did not appear to affect the scores significantly (changes from the default value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does a random forest classifier prevent overfitting better than a single decision tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A random forest classifier prevents overfitting better than a single decision tree because each tree in the forest gets different data, so the features that are detected will be different at the end. Bootstrapping aggregation is when points that are repeated in the data are weighted more when the trees are merged together. This minimizes outliers and prevents overfitting. Each tree also randomly selects features, so the random forest classifier will take the average of all the selections and have the best set of features. This also makes the random forest classifier stronger, so it will not overfit as much as a single decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
